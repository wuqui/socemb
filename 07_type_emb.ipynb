{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-dakota",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-password",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socemb.read_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-length",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECS_DIR = 'data/vecs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = 'askreddit'\n",
    "T_1 = 2010\n",
    "T_2 = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-excess",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMM_1 = 'askaconservative'\n",
    "COMM_2 = 'politics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-chuck",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-terrain",
   "metadata": {},
   "source": [
    "### Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_paths = get_fpaths_yr(YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-arcade",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = read_comm_csvs(f_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-society",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 380964 entries, 0 to 380963\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   body         380964 non-null  string        \n",
      " 1   created_utc  380964 non-null  datetime64[ns]\n",
      " 2   id           380964 non-null  string        \n",
      " 3   subreddit    380964 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(3)\n",
      "memory usage: 11.6 MB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-conference",
   "metadata": {},
   "source": [
    "### Split data (inactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-density",
   "metadata": {},
   "source": [
    "### Split in temporal bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.assign(date = pd.to_datetime(\n",
    "    comments['created_utc'],\n",
    "    errors='coerce'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments.date.dt.year == TIME]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-spanish",
   "metadata": {},
   "source": [
    "### Split by communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments\\\n",
    "    .value_counts('subreddit')\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"politics\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"AskHistorians\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-calgary",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def conv_to_lowerc(doc):\n",
    "    return doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert conv_to_lowerc('Test') == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rm_punct(doc):\n",
    "    return re.sub(r'[^\\w\\s]+', ' ', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rm_punct('No-punctuation!') == 'No punctuation '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenize('These are three-tokens')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def detect_short_doc(doc, limit=10):\n",
    "    if len(doc) >= limit:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei'], 10) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-chrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei', 'viere', 'fuenfe', 'sechse', 'simme', 'achte', 'neine', 'zehne'], 10) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clean_docs(docs):\n",
    "    docs_clean = docs\\\n",
    "        .apply(conv_to_lowerc)\\\n",
    "        .apply(rm_punct)\\\n",
    "        .apply(tokenize)\\\n",
    "        .where(lambda x : x.apply(detect_short_doc) == False)\\\n",
    "        .dropna()    \n",
    "    return docs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = clean_docs(comments['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-multiple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [you, re, asking, how, they, re, going, to, be...\n",
       "1         [gt, i, don, t, think, there, are, any, varyin...\n",
       "2         [its, split, on, copyright, it, leans, anti, n...\n",
       "3         [that, would, be, up, to, the, land, owners, a...\n",
       "4         [i, have, him, here, gun, to, his, head, round...\n",
       "                                ...                        \n",
       "380954    [the, nature, police, are, supposed, to, be, p...\n",
       "380957    [i, got, reddit, last, year, and, only, starte...\n",
       "380958    [i, m, just, curious, how, this, is, clear, to...\n",
       "380962    [staying, at, said, job, while, being, underpa...\n",
       "380963    [i, m, not, sure, that, this, has, ever, been,...\n",
       "Name: body, Length: 264796, dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-worker",
   "metadata": {},
   "source": [
    "### Lexeme-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = docs_clean\\\n",
    "    .str.replace('anglo-saxon', 'anglosaxon')\\\n",
    "    .str.replace('anglo saxon', 'anglosaxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = docs_clean.str.replace('anglosaxon', 'Anglo-Saxon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-minister",
   "metadata": {},
   "source": [
    "### Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs_clean):\n",
    "        self.docs_clean = docs_clean\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-patrick",
   "metadata": {},
   "source": [
    "### Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_emb(corpus, MIN_COUNT=5, SIZE=300, WORKERS=8, WINDOW=3):\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        min_count=MIN_COUNT,\n",
    "        size=SIZE,\n",
    "        workers=WORKERS, \n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-partition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 11.1 s, total: 2min 46s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = train_emb(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-interpretation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['you'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-arena",
   "metadata": {},
   "source": [
    "### Evaluate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protected-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/4732: the\n",
      "1/4732: to\n",
      "2/4732: a\n",
      "3/4732: and\n",
      "4/4732: i\n",
      "5/4732: of\n",
      "6/4732: that\n",
      "7/4732: it\n",
      "8/4732: is\n",
      "9/4732: you\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(wv.index2word):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"{index}/{len(wv.index2word)}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-pioneer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guy', 0.6144881248474121),\n",
       " ('woman', 0.6049772500991821),\n",
       " ('man', 0.5703142881393433),\n",
       " ('politician', 0.5600118637084961),\n",
       " ('girl', 0.5361282229423523),\n",
       " ('candidate', 0.5117915868759155),\n",
       " ('cop', 0.5107803344726562),\n",
       " ('someone', 0.4916432201862335),\n",
       " ('senator', 0.4843234419822693),\n",
       " ('kid', 0.4767877161502838),\n",
       " ('persons', 0.46128833293914795),\n",
       " ('victim', 0.45970290899276733),\n",
       " ('somebody', 0.4551505148410797),\n",
       " ('lady', 0.4502111077308655),\n",
       " ('citizen', 0.44879597425460815),\n",
       " ('owner', 0.44654402136802673),\n",
       " ('mother', 0.44434842467308044),\n",
       " ('parent', 0.44392669200897217),\n",
       " ('fetus', 0.4342397451400757),\n",
       " ('entity', 0.43350768089294434)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('person', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_rel_1 = 'person'\n",
    "lex_rel_2 = 'man'\n",
    "lex_unrel = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-frame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57419455"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_rel = wv.similarity(lex_rel_1, lex_rel_2)\n",
    "sim_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-cradle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27401066"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_1 = wv.similarity(lex_rel_1, lex_unrel)\n",
    "sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-diamond",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14052157"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_2 = wv.similarity(lex_rel_2, lex_unrel)\n",
    "sim_unrel_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-future",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(f'~/promo/socemb/data/vecs/year/{YEAR}.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-lawsuit",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(f'{VECS_DIR}{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-patrol",
   "metadata": {},
   "source": [
    "In the next cell, I need to to use `init_sims` to normalize the vectors. To do this, I need to save and load full models, not just the vector tables. Therefore, I need to train all models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_model(SUBREDDIT: str, YEAR: int, LIMIT: int = 100_000):\n",
    "    \"\"\"Load word2vec model from disk.\"\"\"\n",
    "    model = KeyedVectors.load(f'data/vecs/{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "static-label",
   "metadata": {},
   "source": [
    "model_1 = KeyedVectors.load(f'data/vecs/year/2010.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acceptable-smell",
   "metadata": {},
   "source": [
    "model_2 = KeyedVectors.load(f'data/vecs/year/2020.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEARS = range(2013, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naked-consent",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = load_model(COMM_1, YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = load_model(COMM_2, YEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-telephone",
   "metadata": {},
   "source": [
    "## Extract vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_vec_from_model(lex: str, model):\n",
    "    return model[lex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-plastic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00899458e-01, -1.52170643e-01, -9.39689055e-02, -9.99266282e-03,\n",
       "        8.40886682e-02, -2.21601501e-02, -2.10636541e-01,  3.73399556e-02,\n",
       "       -1.49149716e-01,  2.40157068e-01, -6.44295737e-02,  3.83358356e-03,\n",
       "        1.28691554e-01, -3.57708037e-02,  5.21394191e-03, -1.38501897e-01,\n",
       "       -5.26996851e-02,  1.61609109e-02, -8.78354255e-03,  7.47078285e-02,\n",
       "        1.85247645e-01, -1.60785943e-01,  9.02396999e-03, -8.07802901e-02,\n",
       "        1.38207108e-01,  7.45966509e-02, -8.67893994e-02, -3.15905828e-03,\n",
       "       -9.56856683e-02,  1.56406348e-03, -1.31446823e-01, -1.16315879e-01,\n",
       "       -1.51329443e-01,  3.41142602e-02, -2.11179614e-01,  1.42359763e-01,\n",
       "        4.96119075e-02, -7.68923908e-02, -1.69393018e-01,  6.09558728e-03,\n",
       "       -6.81157708e-02,  3.04796919e-02,  1.52811790e-02,  8.24113786e-02,\n",
       "       -1.50099978e-01, -8.96327049e-02, -8.57340470e-02,  1.70979887e-01,\n",
       "       -1.25112638e-01,  2.76306607e-02,  2.79491618e-02,  9.32807103e-02,\n",
       "       -8.61876681e-02,  1.77944619e-02,  2.96345446e-02,  5.30728213e-02,\n",
       "       -8.04494545e-02, -5.01200259e-02,  1.03507631e-01,  1.71503704e-02,\n",
       "       -1.30950613e-02, -1.35419443e-01, -2.07035676e-01,  2.01879710e-01,\n",
       "        3.25351916e-02, -1.29286498e-01, -1.97531446e-03, -1.24894537e-01,\n",
       "       -4.70467582e-02, -5.72806485e-02,  2.65490144e-01,  1.32199138e-01,\n",
       "       -1.23013787e-01,  1.43621787e-01, -1.69478387e-01, -7.81497061e-02,\n",
       "       -1.00367889e-01, -1.21399298e-01,  7.81577751e-02, -1.83465760e-02,\n",
       "        7.79631063e-02, -1.19734975e-02,  1.60187319e-01, -1.71157122e-01,\n",
       "        7.66789392e-02,  7.91621804e-02,  1.02329522e-01,  3.14248577e-02,\n",
       "       -6.46566320e-03,  7.77441710e-02, -8.66011456e-02,  1.13437630e-01,\n",
       "       -4.57763635e-02,  2.04980507e-01, -7.56074786e-02,  8.36985260e-02,\n",
       "        1.42146304e-01, -6.21182211e-02, -2.30045334e-01,  1.18040875e-01,\n",
       "       -5.66891246e-02, -8.21440369e-02, -4.80789393e-02, -1.77437097e-01,\n",
       "        9.91176143e-02, -3.86907533e-03, -2.86260829e-03, -2.78975964e-02,\n",
       "        5.89535804e-03, -3.08745392e-02,  8.62547010e-02,  9.48113762e-03,\n",
       "        5.90718351e-02,  3.37393098e-02,  3.28736864e-02,  1.05816521e-01,\n",
       "       -1.95885465e-01, -5.06097898e-02,  6.13971949e-02, -4.65643108e-02,\n",
       "        4.25615050e-02, -3.81718166e-02, -1.26955379e-03,  8.49239081e-02,\n",
       "        9.27404463e-02, -1.93988755e-02,  6.38775080e-02,  6.18626922e-02,\n",
       "        1.85889825e-02,  5.36145121e-02, -7.63010159e-02, -2.28864029e-02,\n",
       "        1.12783954e-01, -6.77656010e-02, -5.40199578e-02,  4.24536690e-02,\n",
       "       -2.10998636e-02,  4.41271439e-02, -2.11109277e-02, -4.07798216e-02,\n",
       "       -6.53017089e-02,  8.22475106e-02, -1.27006799e-01,  5.21760248e-02,\n",
       "        7.72923231e-02,  5.92733584e-02, -7.59449974e-02,  1.37864519e-02,\n",
       "        6.56161755e-02,  1.87342602e-03, -2.98692621e-02, -1.71438992e-01,\n",
       "       -2.10373942e-02, -1.79248124e-01,  3.55278850e-02, -1.39222652e-01,\n",
       "       -3.55466902e-02, -7.04349484e-03,  2.28033274e-01,  6.31250069e-02,\n",
       "       -5.19976579e-02, -4.49401466e-03, -6.95441291e-02, -6.49695545e-02,\n",
       "        4.41456810e-02, -6.32601529e-02,  7.57679529e-03, -1.96926054e-02,\n",
       "        7.52070323e-02, -1.74132443e-03, -5.39307296e-02, -7.09064826e-02,\n",
       "        3.26468796e-02,  5.22061251e-02,  5.12205139e-02,  4.76659238e-02,\n",
       "        4.56579439e-02, -6.42024055e-02,  4.80341092e-02,  1.02350175e-01,\n",
       "        2.31694374e-02,  3.88065204e-02,  7.10783824e-02,  2.59597432e-02,\n",
       "       -7.04208985e-02, -5.36137959e-03,  6.84704483e-02,  8.51734355e-02,\n",
       "       -8.39377567e-03,  2.19820887e-02,  3.01600471e-02,  1.38070583e-01,\n",
       "       -1.10092148e-01, -3.17500904e-02, -1.83150789e-03, -4.73540314e-02,\n",
       "        1.40368238e-01,  6.80037290e-02,  9.01781917e-02, -4.44593504e-02,\n",
       "       -1.51467979e-01,  2.42187525e-04, -1.63563807e-02, -2.57572215e-02,\n",
       "       -2.11305395e-02,  5.19033298e-02,  5.04359379e-02, -1.33758308e-02,\n",
       "        6.89290091e-02,  3.82616222e-02,  2.53892485e-02, -1.34443000e-01,\n",
       "        3.35669331e-02, -9.69510302e-02,  1.39094278e-01, -1.65155530e-02,\n",
       "       -1.82096679e-02, -1.11466005e-01, -1.87195148e-02, -8.53353441e-02,\n",
       "       -8.79665278e-03, -1.62471592e-01, -1.18864387e-01, -1.06037967e-01,\n",
       "        2.03552887e-01,  7.81466141e-02, -2.55257934e-02,  4.41919118e-02,\n",
       "       -5.35115274e-03,  7.41289705e-02, -5.00492938e-02, -5.14440984e-02,\n",
       "        5.54420315e-02, -9.08971801e-02, -1.12828091e-01, -2.74523664e-02,\n",
       "        1.00473501e-01, -9.29718390e-02, -2.66576800e-02, -1.88935280e-01,\n",
       "        2.80526970e-02, -2.32480541e-02, -5.77526242e-02, -9.31198150e-03,\n",
       "       -8.35336000e-02, -9.56062749e-02,  4.66225483e-02, -2.34191064e-02,\n",
       "        6.23474680e-02, -5.16520888e-02,  5.65064214e-02,  3.69520336e-02,\n",
       "        6.20320924e-02, -2.31802110e-02,  5.98645248e-02,  1.64918024e-02,\n",
       "       -9.69538540e-02, -6.29603118e-02,  5.67020923e-02, -5.48162777e-03,\n",
       "        5.18470295e-02, -3.28053050e-02,  5.66279180e-02, -8.10693279e-02,\n",
       "       -1.41203394e-02,  4.06210162e-02,  9.53703597e-02,  1.13088809e-01,\n",
       "        1.04754485e-01, -4.08964790e-02,  9.57574025e-02, -6.97239265e-02,\n",
       "        1.65452719e-01, -3.47949490e-02,  5.89585938e-02, -9.36866552e-02,\n",
       "       -1.28151579e-02,  3.97527851e-02,  1.17300674e-01, -9.36452113e-03,\n",
       "       -1.83551311e-02, -7.25350156e-02, -8.76323506e-02,  1.00873560e-01,\n",
       "        3.77283618e-02, -1.09247148e-01, -6.19297586e-02,  8.83863196e-02,\n",
       "       -8.11952353e-02,  9.49463025e-02,  1.85441911e-01,  8.52522478e-02,\n",
       "       -5.40838875e-02, -4.11648862e-02, -2.67577432e-02,  3.95672396e-02,\n",
       "        3.55071016e-02,  1.16477825e-01, -1.97351836e-02, -2.07517445e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vec_from_model(LEX, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-burst",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = model_1['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = model_2['trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-capture",
   "metadata": {},
   "source": [
    "Soc / 2020 / conservative vs. politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-verification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6027644872665405"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(trump_cons_vec, trump_pol_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-taiwan",
   "metadata": {},
   "source": [
    "Soc / 2013 / conservative vs. politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-census",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5683872997760773"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec_1, vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-principle",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-soldier",
   "metadata": {},
   "source": [
    "### For `1` pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t1 = model_1['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t2 = model_2['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t1.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t2.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6892097592353821"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec_t1, vec_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-theorem",
   "metadata": {},
   "source": [
    "### For full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(model_1, model_2):\n",
    "    dist_dict = {}\n",
    "    # will need to replace `vecs_t1.vocab` with a general vocab\n",
    "    for word in model_1.vocab:\n",
    "        if word in model_2.vocab:\n",
    "            dist_dict[word] = cosine(model_1[word], model_2[word])\n",
    "        else:\n",
    "            pass\n",
    "    dist_dict_df = pd.DataFrame(\n",
    "        data=dist_dict.items(), \n",
    "        columns=['word', 'dist']\n",
    "    )\\\n",
    "        .sort_values('dist', ascending=False)\n",
    "    return dist_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = get_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-brave",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>2000</td>\n",
       "      <td>1.094130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8749</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>1.061203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>mitch</td>\n",
       "      <td>1.040116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12751</th>\n",
       "      <td>irreparable</td>\n",
       "      <td>1.026567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12012</th>\n",
       "      <td>2k</td>\n",
       "      <td>1.025281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5045</th>\n",
       "      <td>r</td>\n",
       "      <td>1.010832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>600</td>\n",
       "      <td>1.001934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>georgia</td>\n",
       "      <td>0.998421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11903</th>\n",
       "      <td>moscow</td>\n",
       "      <td>0.993207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4954</th>\n",
       "      <td>mcconnell</td>\n",
       "      <td>0.990447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8871</th>\n",
       "      <td>fuck</td>\n",
       "      <td>0.984308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>somehow</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>pardon</td>\n",
       "      <td>0.978435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5946</th>\n",
       "      <td>permanent</td>\n",
       "      <td>0.974714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>debate</td>\n",
       "      <td>0.973178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>criteria</td>\n",
       "      <td>0.972044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>explicitly</td>\n",
       "      <td>0.969388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>contempt</td>\n",
       "      <td>0.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>forms</td>\n",
       "      <td>0.958226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13061</th>\n",
       "      <td>shitty</td>\n",
       "      <td>0.956934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word      dist\n",
       "415           2000  1.094130\n",
       "8749      kentucky  1.061203\n",
       "2872         mitch  1.040116\n",
       "12751  irreparable  1.026567\n",
       "12012           2k  1.025281\n",
       "5045             r  1.010832\n",
       "706            600  1.001934\n",
       "484        georgia  0.998421\n",
       "11903       moscow  0.993207\n",
       "4954     mcconnell  0.990447\n",
       "8871          fuck  0.984308\n",
       "3754       somehow  0.979167\n",
       "2303        pardon  0.978435\n",
       "5946     permanent  0.974714\n",
       "685         debate  0.973178\n",
       "451       criteria  0.972044\n",
       "2542    explicitly  0.969388\n",
       "3402      contempt  0.964600\n",
       "1245         forms  0.958226\n",
       "13061       shitty  0.956934"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.nlargest(20, 'dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-board",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'moscow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-reset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tennessee          22\n",
      "indiana            30\n",
      "boston             40\n",
      "massachusetts      41\n",
      "hawaii             33\n",
      "philly             30\n",
      "hogan              21\n",
      "mississippi        26\n",
      "1989                8\n",
      "nv                 13\n",
      "\n",
      "---\n",
      "\n",
      "mcconnell        3447\n",
      "mcconnel          171\n",
      "mcconnells         91\n",
      "bluffing            6\n",
      "mconnell           11\n",
      "mcconell           18\n",
      "mcturtle           35\n",
      "blocks            101\n",
      "mitch            3221\n",
      "turtle            249\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-slovak",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-229-26e9e9e88f99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{nb:<15}{model.vocab[nb].count:>6}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/socemb/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mlimited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrestrict_vocab\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimited\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        for nb, dist in model.most_similar(lex):\n",
    "            if model.vocab[nb].count > 300:\n",
    "                print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "                i += 1\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-voltage",
   "metadata": {},
   "source": [
    "### Temporal variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-syndrome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bankrupt\n",
      "absorb\n",
      "distributed\n",
      "detect\n",
      "essentials\n",
      "composite\n",
      "bypass\n",
      "ideally\n",
      "glide\n",
      "prop\n",
      "\n",
      "---\n",
      "\n",
      "biden\n",
      "bernie\n",
      "obama\n",
      "hillary\n",
      "sanders\n",
      "he\n",
      "himself\n",
      "nixon\n",
      "putin\n",
      "pelosi\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(nb)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-sweden",
   "metadata": {},
   "source": [
    "### Social variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'quarantine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askaconservative\n",
      "---\n",
      "ppe\n",
      "rain\n",
      "bombings\n",
      "port\n",
      "aerosols\n",
      "bullets\n",
      "communications\n",
      "missiles\n",
      "weekly\n",
      "2006\n"
     ]
    }
   ],
   "source": [
    "print(f'{COMM_1}\\n---')\n",
    "for nb, dist in model_1.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asklibertarians\n",
      "---\n",
      "defense\n",
      "defence\n",
      "righteous\n",
      "sufficiency\n",
      "determination\n",
      "isolate\n",
      "legally\n",
      "agency\n",
      "permitted\n",
      "explanatory\n"
     ]
    }
   ],
   "source": [
    "print(f'{COMM_2}\\n---')\n",
    "for nb, dist in model_2.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-isolation",
   "metadata": {},
   "source": [
    "## Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-226-3929bd79b83f>:33: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  old_arr = m.syn0norm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-42362f70d51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmart_procrustes_align_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-225-8c764ab7a643>\u001b[0m in \u001b[0;36msmart_procrustes_align_gensim\u001b[0;34m(base_embed, other_embed, words)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# make sure vocabulary and indices are aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0min_base_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_other_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection_align_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# get the embedding matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-3929bd79b83f>\u001b[0m in \u001b[0;36mintersection_align_gensim\u001b[0;34m(m1, m2, words)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-3929bd79b83f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-equality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "\t\"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "\tCode ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "\t\t(With help from William. Thank you!)\n",
    "\tFirst, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "\tThen do the alignment on the other_embed model.\n",
    "\tReplace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "\tReturn other_embed.\n",
    "\tIf `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "\tbase_embed.init_sims()\n",
    "\tother_embed.init_sims()\n",
    "\n",
    "\t# make sure vocabulary and indices are aligned\n",
    "\tin_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "\t# get the embedding matrices\n",
    "\tbase_vecs = in_base_embed.syn0norm\n",
    "\tother_vecs = in_other_embed.syn0norm\n",
    "\n",
    "\t# just a matrix dot product with numpy\n",
    "\tm = other_vecs.T.dot(base_vecs) \n",
    "\t# SVD method from numpy\n",
    "\tu, _, v = np.linalg.svd(m)\n",
    "\t# another matrix operation\n",
    "\tortho = u.dot(v) \n",
    "\t# Replace original array with modified one\n",
    "\t# i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "\tother_embed.syn0norm = other_embed.syn0 = (other_embed.syn0norm).dot(ortho)\n",
    "\treturn other_embed\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-month",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "\t\"\"\"\n",
    "\tIntersect two gensim word2vec models, m1 and m2.\n",
    "\tOnly the shared vocabulary between them is kept.\n",
    "\tIf 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "\tIndices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "\tThese indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "\t\t-- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "\t\t-- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "\tThe .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Get the vocab for each model\n",
    "\tvocab_m1 = set(m1.vocab.keys())\n",
    "\tvocab_m2 = set(m2.vocab.keys())\n",
    "\n",
    "\t# Find the common vocabulary\n",
    "\tcommon_vocab = vocab_m1&vocab_m2\n",
    "\tif words: common_vocab&=set(words)\n",
    "\n",
    "\t# If no alignment necessary because vocab is identical...\n",
    "\tif not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "\t\treturn (m1,m2)\n",
    "\n",
    "\t# Otherwise sort by frequency (summed for both)\n",
    "\tcommon_vocab = list(common_vocab)\n",
    "\tcommon_vocab.sort(key=lambda w: m1.vocab[w].count + m2.vocab[w].count,reverse=True)\n",
    "\n",
    "\t# Then for each model...\n",
    "\tfor m in [m1,m2]:\n",
    "\t\t# Replace old syn0norm array with new one (with common vocab)\n",
    "\t\tindices = [m.vocab[w].index for w in common_vocab]\n",
    "\t\told_arr = m.syn0norm\n",
    "\t\tnew_arr = np.array([old_arr[index] for index in indices])\n",
    "\t\tm.syn0norm = m.syn0 = new_arr\n",
    "\n",
    "\t\t# Replace old vocab dictionary with new one (with common vocab)\n",
    "\t\t# and old index2word with new one\n",
    "\t\tm.index2word = common_vocab\n",
    "\t\told_vocab = m.vocab\n",
    "\t\tnew_vocab = {}\n",
    "\t\tfor new_index,word in enumerate(common_vocab):\n",
    "\t\t\told_vocab_obj=old_vocab[word]\n",
    "\t\t\tnew_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "\t\tm.vocab = new_vocab\n",
    "\n",
    "\treturn (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = model_1\n",
    "m2 = model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-ferry",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_m1 = set(m1.vocab.keys())\n",
    "vocab_m2 = set(m2.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_vocab = vocab_m1 & vocab_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_vocab = list(common_vocab)\n",
    "common_vocab.sort(key=lambda w: m1.vocab[w].count + m2.vocab[w].count,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-harris",
   "metadata": {},
   "source": [
    "Note that for the new Gensim versions, calls for .index2word, .vocab, .syn0 and .syn0norm should be replaced with .wv.index2word, .wv.vocab, .wv.syn0 and .wv.syn0norm respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-luther",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-255-02a3945973d7>:4: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  old_arr = m.syn0norm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-02a3945973d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-02a3945973d7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "for m in [m1,m2]:\n",
    "    # Replace old syn0norm array with new one (with common vocab)\n",
    "    indices = [m.vocab[w].index for w in common_vocab]\n",
    "    old_arr = m.syn0norm\n",
    "    new_arr = np.array([old_arr[index] for index in indices])\n",
    "    m.syn0norm = m.syn0 = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-material",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-254-e3f9c0e261ee>:2: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  m.syn0norm = m.syn0 = new_arr\n",
      "<ipython-input-254-e3f9c0e261ee>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  m.syn0norm = m.syn0 = new_arr\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Replace old vocab dictionary with new one (with common vocab)\n",
    "    # and old index2word with new one\n",
    "    m.index2word = common_vocab\n",
    "    old_vocab = m.vocab\n",
    "    new_vocab = {}\n",
    "    for new_index,word in enumerate(common_vocab):\n",
    "        old_vocab_obj=old_vocab[word]\n",
    "        new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "    m.vocab = new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-prescription",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gensim_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Returns the aligned/intersected models from a list of gensim word2vec models.\n",
    "    Generalized from original two-way intersection as seen above.\n",
    "    \n",
    "    Also updated to work with the most recent version of gensim\n",
    "    Requires reduce from functools\n",
    "    \n",
    "    In order to run this, make sure you run 'model.init_sims()' for each model before you input them for alignment.\n",
    "    \n",
    "    ##############################################\n",
    "    ORIGINAL DESCRIPTION\n",
    "    ##############################################\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.vocab.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab and not vocab_m3-common_vocab:\n",
    "    #   return (m1,m2,m3)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: sum([m.vocab[w].count for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.vocab[w].index for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.vectors_norm\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.vectors_norm = m.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.init_sims()\n",
    "model_2.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-world",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-b5df47797e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malign_gensim_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-258-a84bdbe9e880>\u001b[0m in \u001b[0;36malign_gensim_models\u001b[0;34m(models, words)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-258-a84bdbe9e880>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "align_gensim_models([model_1, model_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-valley",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-protection",
   "metadata": {},
   "source": [
    "### Semantic change / neology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-passage",
   "metadata": {},
   "source": [
    "#### 2010 vs. 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-thirty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3368</th>\n",
       "      <td>theonion</td>\n",
       "      <td>1.073174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14398</th>\n",
       "      <td>trump</td>\n",
       "      <td>1.064995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24331</th>\n",
       "      <td>pronouns</td>\n",
       "      <td>1.063726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5941</th>\n",
       "      <td>quarantine</td>\n",
       "      <td>1.026087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20712</th>\n",
       "      <td>autonomy</td>\n",
       "      <td>1.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>chat</td>\n",
       "      <td>0.993065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19200</th>\n",
       "      <td>ccp</td>\n",
       "      <td>0.990134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>guardian</td>\n",
       "      <td>0.988905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23239</th>\n",
       "      <td>lockdown</td>\n",
       "      <td>0.987755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16618</th>\n",
       "      <td>230</td>\n",
       "      <td>0.987645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22663</th>\n",
       "      <td>pronoun</td>\n",
       "      <td>0.979626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>cancel</td>\n",
       "      <td>0.975683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20920</th>\n",
       "      <td>absolved</td>\n",
       "      <td>0.967182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>sub</td>\n",
       "      <td>0.964382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6053</th>\n",
       "      <td>messaging</td>\n",
       "      <td>0.961209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>bot</td>\n",
       "      <td>0.961136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>https</td>\n",
       "      <td>0.961055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>pardon</td>\n",
       "      <td>0.959310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21526</th>\n",
       "      <td>kink</td>\n",
       "      <td>0.957009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17816</th>\n",
       "      <td>frontline</td>\n",
       "      <td>0.950385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word      dist\n",
       "3368     theonion  1.073174\n",
       "14398       trump  1.064995\n",
       "24331    pronouns  1.063726\n",
       "5941   quarantine  1.026087\n",
       "20712    autonomy  1.000578\n",
       "2834         chat  0.993065\n",
       "19200         ccp  0.990134\n",
       "3474     guardian  0.988905\n",
       "23239    lockdown  0.987755\n",
       "16618         230  0.987645\n",
       "22663     pronoun  0.979626\n",
       "6978       cancel  0.975683\n",
       "20920    absolved  0.967182\n",
       "1758          sub  0.964382\n",
       "6053    messaging  0.961209\n",
       "2473          bot  0.961136\n",
       "1231        https  0.961055\n",
       "1689       pardon  0.959310\n",
       "21526        kink  0.957009\n",
       "17816   frontline  0.950385"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.nlargest(20, 'dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-passenger",
   "metadata": {},
   "source": [
    "#### 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-chester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatten              5\n",
      "revert             19\n",
      "seize              33\n",
      "flare              14\n",
      "tighten            20\n",
      "spark              44\n",
      "dissolve           31\n",
      "freeze             64\n",
      "stifle             32\n",
      "clog                9\n",
      "\n",
      "---\n",
      "\n",
      "biden            8867\n",
      "obama            3149\n",
      "trumps            843\n",
      "he              62307\n",
      "putin             315\n",
      "djt                70\n",
      "hillary           896\n",
      "bernie           2856\n",
      "him             17557\n",
      "jojo              110\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-replacement",
   "metadata": {},
   "source": [
    "## Obsolete stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-hacker",
   "metadata": {},
   "source": [
    "### Gensim preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "loved-worcester",
   "metadata": {},
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short"
   ]
  },
  {
   "cell_type": "raw",
   "id": "forty-manufacturer",
   "metadata": {},
   "source": [
    "# docs_clean = preprocess_documents(docs)\n",
    "# docs_clean = stem_text()\n",
    "#docs_clean = strip_tags(docs)\n",
    "docs_clean = strip_punctuation(docs_clean)\n",
    "docs_clean = strip_multiple_whitespaces(docs_clean)\n",
    "docs_clean = strip_numeric(docs_clean)\n",
    "docs_clean = remove_stopwords(docs_clean)\n",
    "docs_clean = strip_short(docs_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-deputy",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-ground",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socemb.read_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-audience",
   "metadata": {},
   "source": [
    "## Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path_1 = get_fpath_subr_yr('askaconservative', 100_000, 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path_2 = get_fpath_subr_yr('askaconservative', 100_000, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-public",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_1 = read_comm_csv(f_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_2 = read_comm_csv(f_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-criticism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97243 entries, 0 to 97242\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   body         97243 non-null  string        \n",
      " 1   created_utc  97243 non-null  datetime64[ns]\n",
      " 2   id           97243 non-null  string        \n",
      " 3   subreddit    97243 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(3)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "comments_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-connecticut",
   "metadata": {},
   "source": [
    "## Split comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "other-welcome",
   "metadata": {},
   "source": [
    "### Split in temporal bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.assign(date = pd.to_datetime(\n",
    "    comments['created_utc'],\n",
    "    errors='coerce'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments.date.dt.year == TIME]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-empire",
   "metadata": {},
   "source": [
    "### Split by communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-field",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments\\\n",
    "    .value_counts('subreddit')\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"politics\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"AskHistorians\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-spray",
   "metadata": {},
   "source": [
    "## Pre-process comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def conv_to_lowerc(doc):\n",
    "    return doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert conv_to_lowerc('Test') == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rm_punct(doc):\n",
    "    return re.sub(r'[^\\w\\s]+', ' ', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rm_punct('No-punctuation!') == 'No punctuation '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenize('These are three-tokens')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def detect_short_doc(doc, limit=10):\n",
    "    if len(doc) >= limit:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei'], 10) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei', 'viere', 'fuenfe', 'sechse', 'simme', 'achte', 'neine', 'zehne'], 10) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-currency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clean_docs(docs):\n",
    "    docs_clean = docs\\\n",
    "        .apply(conv_to_lowerc)\\\n",
    "        .apply(rm_punct)\\\n",
    "        .apply(tokenize)\\\n",
    "        .where(lambda x : x.apply(detect_short_doc) == False)\\\n",
    "        .dropna()    \n",
    "    return docs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-canal",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean_1 = clean_docs(comments_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean_2 = clean_docs(comments_2['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-assurance",
   "metadata": {},
   "source": [
    "## Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs_clean):\n",
    "        self.docs_clean = docs_clean\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1 = Corpus(docs_clean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_2 = Corpus(docs_clean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-opera",
   "metadata": {},
   "source": [
    "## Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_emb(corpus, MIN_COUNT=5, SIZE=300, WORKERS=8, WINDOW=3):\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        min_count=MIN_COUNT,\n",
    "        size=SIZE,\n",
    "        workers=WORKERS, \n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = train_emb(corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-shield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.2 s, sys: 253 ms, total: 45.5 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_2 = train_emb(corpus_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-charles",
   "metadata": {},
   "source": [
    "## Evaluate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-means",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20738: the\n",
      "1/20738: to\n",
      "2/20738: and\n",
      "3/20738: a\n",
      "4/20738: of\n",
      "5/20738: that\n",
      "6/20738: is\n",
      "7/20738: i\n",
      "8/20738: it\n",
      "9/20738: in\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(model_1.wv.index2word):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"{index}/{len(model_1.wv.index2word)}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-secretariat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.7169589400291443),\n",
       " ('man', 0.6905370950698853),\n",
       " ('guy', 0.6834359169006348),\n",
       " ('baby', 0.6324769258499146),\n",
       " ('someone', 0.6222299337387085),\n",
       " ('doctor', 0.6170642971992493),\n",
       " ('kid', 0.6133965253829956),\n",
       " ('parent', 0.6090949773788452),\n",
       " ('anyone', 0.596699595451355),\n",
       " ('politician', 0.5884082317352295),\n",
       " ('fetus', 0.5837575793266296),\n",
       " ('child', 0.5680943727493286),\n",
       " ('somebody', 0.5629948377609253),\n",
       " ('mother', 0.5546822547912598),\n",
       " ('user', 0.534717857837677),\n",
       " ('friend', 0.5345014333724976),\n",
       " ('people', 0.5293498039245605),\n",
       " ('victim', 0.5277709364891052),\n",
       " ('citizen', 0.5250868201255798),\n",
       " ('president', 0.520645022392273)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.wv.most_similar('person', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_rel_1 = 'person'\n",
    "lex_rel_2 = 'man'\n",
    "lex_unrel = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-extreme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69053704"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_rel = model_1.wv.similarity(lex_rel_1, lex_rel_2)\n",
    "sim_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-minister",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18910645"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_1 = model_1.wv.similarity(lex_rel_1, lex_unrel)\n",
    "sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-suffering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12229807"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_2 = model_1.wv.similarity(lex_rel_2, lex_unrel)\n",
    "sim_unrel_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-canberra",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-moment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.wv.save(f'~/promo/socemb/data/vecs/year/{YEAR}.model_1.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-telling",
   "metadata": {},
   "source": [
    "## Load models (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(f'{VECS_DIR}{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-independence",
   "metadata": {},
   "source": [
    "In the next cell, I need to to use `init_sims` to normalize the vectors. To do this, I need to save and load full models, not just the vector tables. Therefore, I need to train all models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_model(SUBREDDIT: str, YEAR: int, LIMIT: int = 100_000):\n",
    "    \"\"\"Load word2vec model from disk.\"\"\"\n",
    "    model = KeyedVectors.load(f'data/vecs/{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aerial-bride",
   "metadata": {},
   "source": [
    "model_1 = KeyedVectors.load(f'data/vecs/year/2010.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "approved-bleeding",
   "metadata": {},
   "source": [
    "model_2 = KeyedVectors.load(f'data/vecs/year/2020.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = load_model(COMM_1, YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = load_model(COMM_2, YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_vec_from_model(lex: str, model):\n",
    "    return model[lex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_vec_from_model(LEX, model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 = model_1['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_2 = model_2['trump']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-hands",
   "metadata": {},
   "source": [
    "Soc / 2020 / conservative vs. politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-craft",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(trump_cons_vec, trump_pol_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-calgary",
   "metadata": {},
   "source": [
    "Soc / 2013 / conservative vs. politics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec_1, vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-colorado",
   "metadata": {},
   "source": [
    "## Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gensim_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Returns the aligned/intersected models from a list of gensim word2vec models.\n",
    "    Generalized from original two-way intersection as seen above.\n",
    "    \n",
    "    Also updated to work with the most recent version of gensim\n",
    "    Requires reduce from functools\n",
    "    \n",
    "    In order to run this, make sure you run 'model.init_sims()' for each model before you input them for alignment.\n",
    "    \n",
    "    ##############################################\n",
    "    ORIGINAL DESCRIPTION\n",
    "    ##############################################\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.vocab.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab and not vocab_m3-common_vocab:\n",
    "    #   return (m1,m2,m3)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: sum([m.vocab[w].count for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.vocab[w].index for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.vectors_norm\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.vectors_norm = m.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.init_sims(replace=True)\n",
    "model_2.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-37-a84bdbe9e880>:53: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  m.vectors_norm = m.syn0 = new_arr\n"
     ]
    }
   ],
   "source": [
    "models_ali = align_gensim_models([model_1.wv, model_2.wv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_ali = models_ali[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_ali = models_ali[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-intellectual",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-loading",
   "metadata": {},
   "source": [
    "### For `1` pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t1 = model_1['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t2 = model_2['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t1.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t2.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec_t1, vec_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-photographer",
   "metadata": {},
   "source": [
    "### For full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(model_1, model_2):\n",
    "    dist_dict = {}\n",
    "    # will need to replace `vecs_t1.vocab` with a general vocab\n",
    "    for word in model_1.wv.vocab:\n",
    "        if word in model_2.wv.vocab:\n",
    "            dist_dict[word] = cosine(model_1[word], model_2[word])\n",
    "        else:\n",
    "            pass\n",
    "    dist_dict_df = pd.DataFrame(\n",
    "        data=dist_dict.items(), \n",
    "        columns=['word', 'dist']\n",
    "    )\\\n",
    "        .sort_values('dist', ascending=False)\n",
    "    return dist_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-682d38aa6c6f>:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  dist_dict[word] = cosine(model_1[word], model_2[word])\n"
     ]
    }
   ],
   "source": [
    "dists = get_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-electric",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>virus</td>\n",
       "      <td>0.988359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>masks</td>\n",
       "      <td>0.919138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>mask</td>\n",
       "      <td>0.890782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>anyways</td>\n",
       "      <td>0.839805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>treatise</td>\n",
       "      <td>0.818108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15832</th>\n",
       "      <td>pertain</td>\n",
       "      <td>0.813345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4855</th>\n",
       "      <td>cloth</td>\n",
       "      <td>0.812976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2418</th>\n",
       "      <td>flu</td>\n",
       "      <td>0.812539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>x200b</td>\n",
       "      <td>0.811742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7472</th>\n",
       "      <td>reopen</td>\n",
       "      <td>0.791699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>cancel</td>\n",
       "      <td>0.791181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2723</th>\n",
       "      <td>vaccine</td>\n",
       "      <td>0.790368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13084</th>\n",
       "      <td>sunk</td>\n",
       "      <td>0.783152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11822</th>\n",
       "      <td>bribed</td>\n",
       "      <td>0.778805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15482</th>\n",
       "      <td>characterizing</td>\n",
       "      <td>0.777605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15799</th>\n",
       "      <td>hemorrhaging</td>\n",
       "      <td>0.772076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>divorce</td>\n",
       "      <td>0.770271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1775</th>\n",
       "      <td>disease</td>\n",
       "      <td>0.769602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9420</th>\n",
       "      <td>plumber</td>\n",
       "      <td>0.767106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>peaceful</td>\n",
       "      <td>0.766152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word      dist\n",
       "1000            virus  0.988359\n",
       "907             masks  0.919138\n",
       "792              mask  0.890782\n",
       "2554          anyways  0.839805\n",
       "13655        treatise  0.818108\n",
       "15832         pertain  0.813345\n",
       "4855            cloth  0.812976\n",
       "2418              flu  0.812539\n",
       "435             x200b  0.811742\n",
       "7472           reopen  0.791699\n",
       "2330           cancel  0.791181\n",
       "2723          vaccine  0.790368\n",
       "13084            sunk  0.783152\n",
       "11822          bribed  0.778805\n",
       "15482  characterizing  0.777605\n",
       "15799    hemorrhaging  0.772076\n",
       "2819          divorce  0.770271\n",
       "1775          disease  0.769602\n",
       "9420          plumber  0.767106\n",
       "1611         peaceful  0.766152"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.nlargest(20, 'dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-offense",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMM_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(LEX, topn=20):\n",
    "        print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        for nb, dist in model.most_similar(lex):\n",
    "            if model.vocab[nb].count > 300:\n",
    "                print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "                i += 1\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-factor",
   "metadata": {},
   "source": [
    "### Temporal variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(nb)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-platinum",
   "metadata": {},
   "source": [
    "### Social variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-financing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'quarantine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{COMM_1}\\n---')\n",
    "for nb, dist in model_1.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-foundation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{COMM_2}\\n---')\n",
    "for nb, dist in model_2.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-alert",
   "metadata": {},
   "source": [
    "## Obsolete stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-jerusalem",
   "metadata": {},
   "source": [
    "### Extract vector for target word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powerful-investor",
   "metadata": {},
   "source": [
    "### Gensim preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "close-insider",
   "metadata": {},
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short"
   ]
  },
  {
   "cell_type": "raw",
   "id": "medical-proposition",
   "metadata": {},
   "source": [
    "# docs_clean = preprocess_documents(docs)\n",
    "# docs_clean = stem_text()\n",
    "#docs_clean = strip_tags(docs)\n",
    "docs_clean = strip_punctuation(docs_clean)\n",
    "docs_clean = strip_multiple_whitespaces(docs_clean)\n",
    "docs_clean = strip_numeric(docs_clean)\n",
    "docs_clean = remove_stopwords(docs_clean)\n",
    "docs_clean = strip_short(docs_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-victoria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-north",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-enough",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socemb.read_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-dividend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-parent",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECS_DIR = 'data/vecs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_TYPE = 'social'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDIT = 'askreddit'\n",
    "T_1 = 2010\n",
    "T_2 = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMM_1 = 'askaconservative'\n",
    "COMM_2 = 'asklibertarians'\n",
    "YEAR = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-citation",
   "metadata": {},
   "source": [
    "## Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = get_fpath_subr_yr(SUBREDDIT, LIMIT, YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = read_comm_csv(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-heaven",
   "metadata": {},
   "source": [
    "## Split data (inactive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-sector",
   "metadata": {},
   "source": [
    "### Split in temporal bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.assign(date = pd.to_datetime(\n",
    "    comments['created_utc'],\n",
    "    errors='coerce'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments.date.dt.year == TIME]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-fountain",
   "metadata": {},
   "source": [
    "### Split by communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments\\\n",
    "    .value_counts('subreddit')\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"politics\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"AskHistorians\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-vitamin",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def conv_to_lowerc(doc):\n",
    "    return doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert conv_to_lowerc('Test') == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rm_punct(doc):\n",
    "    return re.sub(r'[^\\w\\s]+', ' ', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rm_punct('No-punctuation!') == 'No punctuation '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenize('These are three-tokens')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def detect_short_doc(doc, limit=10):\n",
    "    if len(doc) >= limit:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei'], 10) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei', 'viere', 'fuenfe', 'sechse', 'simme', 'achte', 'neine', 'zehne'], 10) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clean_docs(docs):\n",
    "    docs_clean = docs\\\n",
    "        .apply(conv_to_lowerc)\\\n",
    "        .apply(rm_punct)\\\n",
    "        .apply(tokenize)\\\n",
    "        .where(lambda x : x.apply(detect_short_doc) == False)\\\n",
    "        .dropna()    \n",
    "    return docs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = clean_docs(comments['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-hanging",
   "metadata": {},
   "source": [
    "### Lexeme-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = docs_clean\\\n",
    "    .str.replace('anglo-saxon', 'anglosaxon')\\\n",
    "    .str.replace('anglo saxon', 'anglosaxon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean = docs_clean.str.replace('anglosaxon', 'Anglo-Saxon')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-charlotte",
   "metadata": {},
   "source": [
    "## Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs_clean):\n",
    "        self.docs_clean = docs_clean\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-attachment",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(docs_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-silence",
   "metadata": {},
   "source": [
    "## Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-christopher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_emb(corpus, MIN_COUNT=5, SIZE=300, WORKERS=8, WINDOW=3):\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        min_count=MIN_COUNT,\n",
    "        size=SIZE,\n",
    "        workers=WORKERS, \n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = train_emb(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv['you'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-reminder",
   "metadata": {},
   "source": [
    "## Evaluate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index2word):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"{index}/{len(wv.index2word)}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('person', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_rel_1 = 'person'\n",
    "lex_rel_2 = 'man'\n",
    "lex_unrel = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_rel = wv.similarity(lex_rel_1, lex_rel_2)\n",
    "sim_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_unrel_1 = wv.similarity(lex_rel_1, lex_unrel)\n",
    "sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-minority",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_unrel_2 = wv.similarity(lex_rel_2, lex_unrel)\n",
    "sim_unrel_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-development",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiac-belief",
   "metadata": {},
   "source": [
    "## Save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.save(f'{VECS_DIR}{SUBREDDIT}_{YEAR}_{LIMIT}.wv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-florida",
   "metadata": {},
   "source": [
    "## Load vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(f'{VECS_DIR}{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-intranet",
   "metadata": {},
   "source": [
    "In the next cell, I need to to use `init_sims` to normalize the vectors. To do this, I need to save and load full models, not just the vector tables. Therefore, I need to train all models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(SUBREDDIT, YEAR, LIMIT):\n",
    "    model = KeyedVectors.load(f'data/vecs/{SUBREDDIT}_{YEAR}_{LIMIT}.wv', mmap='r')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = KeyedVectors.load(f'data/vecs/year/2010.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = KeyedVectors.load(f'data/vecs/year/2020.wv', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-edition",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-market",
   "metadata": {},
   "source": [
    "### For `1` pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t1 = vecs_t1['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t2 = vecs_t2['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-funeral",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t1.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t2.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-appreciation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6213354766368866"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(vec_t1, vec_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-timing",
   "metadata": {},
   "source": [
    "### For full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-prototype",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(model_1, model_2):\n",
    "    dist_dict = {}\n",
    "    # will need to replace `vecs_t1.vocab` with a general vocab\n",
    "    for word in model_1.vocab:\n",
    "        if word in model_2.vocab:\n",
    "            dist_dict[word] = cosine(model_1[word], model_2[word])\n",
    "        else:\n",
    "            pass\n",
    "    dist_dict_df = pd.DataFrame(\n",
    "        data=dist_dict.items(), \n",
    "        columns=['word', 'dist']\n",
    "    )\\\n",
    "        .sort_values('dist', ascending=False)\n",
    "    return dist_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-thumbnail",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = get_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>that</td>\n",
       "      <td>1.053959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12078</th>\n",
       "      <td>mitch</td>\n",
       "      <td>0.956240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13711</th>\n",
       "      <td>stressed</td>\n",
       "      <td>0.938686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6964</th>\n",
       "      <td>pardons</td>\n",
       "      <td>0.927164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6551</th>\n",
       "      <td>adhd</td>\n",
       "      <td>0.923108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15113</th>\n",
       "      <td>2k</td>\n",
       "      <td>0.918534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085</th>\n",
       "      <td>pardon</td>\n",
       "      <td>0.914406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5233</th>\n",
       "      <td>filibuster</td>\n",
       "      <td>0.913616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>actually</td>\n",
       "      <td>0.912229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16466</th>\n",
       "      <td>kentucky</td>\n",
       "      <td>0.904231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12576</th>\n",
       "      <td>shower</td>\n",
       "      <td>0.892698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7308</th>\n",
       "      <td>pray</td>\n",
       "      <td>0.891886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>cuz</td>\n",
       "      <td>0.890170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>nails</td>\n",
       "      <td>0.887633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>remembered</td>\n",
       "      <td>0.883931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>flipped</td>\n",
       "      <td>0.881784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13845</th>\n",
       "      <td>2022</td>\n",
       "      <td>0.877658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13976</th>\n",
       "      <td>deaf</td>\n",
       "      <td>0.877284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>0.876200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9087</th>\n",
       "      <td>researched</td>\n",
       "      <td>0.873359</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word      dist\n",
       "87           that  1.053959\n",
       "12078       mitch  0.956240\n",
       "13711    stressed  0.938686\n",
       "6964      pardons  0.927164\n",
       "6551         adhd  0.923108\n",
       "15113          2k  0.918534\n",
       "6085       pardon  0.914406\n",
       "5233   filibuster  0.913616\n",
       "53       actually  0.912229\n",
       "16466    kentucky  0.904231\n",
       "12576      shower  0.892698\n",
       "7308         pray  0.891886\n",
       "4416          cuz  0.890170\n",
       "9875        nails  0.887633\n",
       "3022   remembered  0.883931\n",
       "10372     flipped  0.881784\n",
       "13845        2022  0.877658\n",
       "13976        deaf  0.877284\n",
       "3201   enthusiasm  0.876200\n",
       "9087   researched  0.873359"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.nlargest(20, 'dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-shape",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'pardons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statements        604\n",
      "crimes           1746\n",
      "allegations       224\n",
      "arrests           134\n",
      "convictions       114\n",
      "appointees         66\n",
      "witnesses         211\n",
      "acts              634\n",
      "charges           667\n",
      "pardon           1009\n",
      "\n",
      "---\n",
      "\n",
      "dares               6\n",
      "barrett            10\n",
      "nominations         7\n",
      "th                  5\n",
      "maliciously         8\n",
      "profiles            6\n",
      "resisting          28\n",
      "facist              6\n",
      "delegates          20\n",
      "byrd                5\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-steel",
   "metadata": {},
   "source": [
    "### Temporal variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-teddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-breed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lift\n",
      "spark\n",
      "tighten\n",
      "stifle\n",
      "weaken\n",
      "override\n",
      "alleviate\n",
      "ramp\n",
      "wash\n",
      "enhance\n",
      "\n",
      "---\n",
      "\n",
      "biden\n",
      "bernie\n",
      "obama\n",
      "himself\n",
      "he\n",
      "sanders\n",
      "hillary\n",
      "him\n",
      "nixon\n",
      "putin\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in [vecs_t1, vecs_t2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(nb)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-exploration",
   "metadata": {},
   "source": [
    "### Social variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'quarantine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "askaconservative\n",
      "---\n",
      "ppe\n",
      "rain\n",
      "bombings\n",
      "port\n",
      "aerosols\n",
      "bullets\n",
      "communications\n",
      "missiles\n",
      "weekly\n",
      "2006\n"
     ]
    }
   ],
   "source": [
    "print(f'{COMM_1}\\n---')\n",
    "for nb, dist in model_1.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asklibertarians\n",
      "---\n",
      "defense\n",
      "defence\n",
      "righteous\n",
      "sufficiency\n",
      "determination\n",
      "isolate\n",
      "legally\n",
      "agency\n",
      "permitted\n",
      "explanatory\n"
     ]
    }
   ],
   "source": [
    "print(f'{COMM_2}\\n---')\n",
    "for nb, dist in model_2.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-standard",
   "metadata": {},
   "source": [
    "## Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-member",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-226-3929bd79b83f>:33: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  old_arr = m.syn0norm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-42362f70d51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmart_procrustes_align_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-225-8c764ab7a643>\u001b[0m in \u001b[0;36msmart_procrustes_align_gensim\u001b[0;34m(base_embed, other_embed, words)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# make sure vocabulary and indices are aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0min_base_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_other_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintersection_align_gensim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# get the embedding matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-3929bd79b83f>\u001b[0m in \u001b[0;36mintersection_align_gensim\u001b[0;34m(m1, m2, words)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-226-3929bd79b83f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                 \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "smart_procrustes_align_gensim(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-million",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "\t\"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "\tCode ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "\t\t(With help from William. Thank you!)\n",
    "\tFirst, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "\tThen do the alignment on the other_embed model.\n",
    "\tReplace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "\tReturn other_embed.\n",
    "\tIf `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# patch by Richard So [https://twitter.com/richardjeanso) (thanks!) to update this code for new version of gensim\n",
    "\tbase_embed.init_sims()\n",
    "\tother_embed.init_sims()\n",
    "\n",
    "\t# make sure vocabulary and indices are aligned\n",
    "\tin_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "\t# get the embedding matrices\n",
    "\tbase_vecs = in_base_embed.syn0norm\n",
    "\tother_vecs = in_other_embed.syn0norm\n",
    "\n",
    "\t# just a matrix dot product with numpy\n",
    "\tm = other_vecs.T.dot(base_vecs) \n",
    "\t# SVD method from numpy\n",
    "\tu, _, v = np.linalg.svd(m)\n",
    "\t# another matrix operation\n",
    "\tortho = u.dot(v) \n",
    "\t# Replace original array with modified one\n",
    "\t# i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "\tother_embed.syn0norm = other_embed.syn0 = (other_embed.syn0norm).dot(ortho)\n",
    "\treturn other_embed\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "\t\"\"\"\n",
    "\tIntersect two gensim word2vec models, m1 and m2.\n",
    "\tOnly the shared vocabulary between them is kept.\n",
    "\tIf 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "\tIndices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "\tThese indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "\t\t-- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "\t\t-- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "\tThe .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Get the vocab for each model\n",
    "\tvocab_m1 = set(m1.vocab.keys())\n",
    "\tvocab_m2 = set(m2.vocab.keys())\n",
    "\n",
    "\t# Find the common vocabulary\n",
    "\tcommon_vocab = vocab_m1&vocab_m2\n",
    "\tif words: common_vocab&=set(words)\n",
    "\n",
    "\t# If no alignment necessary because vocab is identical...\n",
    "\tif not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "\t\treturn (m1,m2)\n",
    "\n",
    "\t# Otherwise sort by frequency (summed for both)\n",
    "\tcommon_vocab = list(common_vocab)\n",
    "\tcommon_vocab.sort(key=lambda w: m1.vocab[w].count + m2.vocab[w].count,reverse=True)\n",
    "\n",
    "\t# Then for each model...\n",
    "\tfor m in [m1,m2]:\n",
    "\t\t# Replace old syn0norm array with new one (with common vocab)\n",
    "\t\tindices = [m.vocab[w].index for w in common_vocab]\n",
    "\t\told_arr = m.syn0norm\n",
    "\t\tnew_arr = np.array([old_arr[index] for index in indices])\n",
    "\t\tm.syn0norm = m.syn0 = new_arr\n",
    "\n",
    "\t\t# Replace old vocab dictionary with new one (with common vocab)\n",
    "\t\t# and old index2word with new one\n",
    "\t\tm.index2word = common_vocab\n",
    "\t\told_vocab = m.vocab\n",
    "\t\tnew_vocab = {}\n",
    "\t\tfor new_index,word in enumerate(common_vocab):\n",
    "\t\t\told_vocab_obj=old_vocab[word]\n",
    "\t\t\tnew_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "\t\tm.vocab = new_vocab\n",
    "\n",
    "\treturn (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = model_1\n",
    "m2 = model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_m1 = set(m1.vocab.keys())\n",
    "vocab_m2 = set(m2.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_vocab = vocab_m1 & vocab_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_vocab = list(common_vocab)\n",
    "common_vocab.sort(key=lambda w: m1.vocab[w].count + m2.vocab[w].count,reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-investor",
   "metadata": {},
   "source": [
    "Note that for the new Gensim versions, calls for .index2word, .vocab, .syn0 and .syn0norm should be replaced with .wv.index2word, .wv.vocab, .wv.syn0 and .wv.syn0norm respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-benefit",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-255-02a3945973d7>:4: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  old_arr = m.syn0norm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-255-02a3945973d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-255-02a3945973d7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommon_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "for m in [m1,m2]:\n",
    "    # Replace old syn0norm array with new one (with common vocab)\n",
    "    indices = [m.vocab[w].index for w in common_vocab]\n",
    "    old_arr = m.syn0norm\n",
    "    new_arr = np.array([old_arr[index] for index in indices])\n",
    "    m.syn0norm = m.syn0 = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-empty",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-254-e3f9c0e261ee>:2: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  m.syn0norm = m.syn0 = new_arr\n",
      "<ipython-input-254-e3f9c0e261ee>:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  m.syn0norm = m.syn0 = new_arr\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Replace old vocab dictionary with new one (with common vocab)\n",
    "    # and old index2word with new one\n",
    "    m.index2word = common_vocab\n",
    "    old_vocab = m.vocab\n",
    "    new_vocab = {}\n",
    "    for new_index,word in enumerate(common_vocab):\n",
    "        old_vocab_obj=old_vocab[word]\n",
    "        new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "    m.vocab = new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gensim_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Returns the aligned/intersected models from a list of gensim word2vec models.\n",
    "    Generalized from original two-way intersection as seen above.\n",
    "    \n",
    "    Also updated to work with the most recent version of gensim\n",
    "    Requires reduce from functools\n",
    "    \n",
    "    In order to run this, make sure you run 'model.init_sims()' for each model before you input them for alignment.\n",
    "    \n",
    "    ##############################################\n",
    "    ORIGINAL DESCRIPTION\n",
    "    ##############################################\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.vocab.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab and not vocab_m3-common_vocab:\n",
    "    #   return (m1,m2,m3)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: sum([m.vocab[w].count for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.vocab[w].index for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.vectors_norm\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.vectors_norm = m.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.init_sims()\n",
    "model_2.init_sims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-christmas",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 18164 is out of bounds for axis 0 with size 17915",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-268-b5df47797e21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0malign_gensim_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-258-a84bdbe9e880>\u001b[0m in \u001b[0;36malign_gensim_models\u001b[0;34m(models, words)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-258-a84bdbe9e880>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mold_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mnew_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mold_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 18164 is out of bounds for axis 0 with size 17915"
     ]
    }
   ],
   "source": [
    "align_gensim_models([model_1, model_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-bearing",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-respect",
   "metadata": {},
   "source": [
    "### 'woke'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-camping",
   "metadata": {},
   "source": [
    "#### Nearest semantic neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2012.wv.most_similar(LEX)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2013.wv.most_similar(LEX)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-transportation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2020.wv.most_similar(LEX)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-pepper",
   "metadata": {},
   "source": [
    "#### Semantic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "woke_2012 = model_2012.wv.get_vector('woke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "woke_2013 = model_2013.wv.get_vector('woke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "woke_2020 = model_2020.wv.get_vector('woke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(woke_2012.reshape(1, -1), woke_2020.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-commercial",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(woke_2012.reshape(1, -1), woke_2013.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-packaging",
   "metadata": {},
   "source": [
    "### 'Anglo-Saxon'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-pierre",
   "metadata": {},
   "source": [
    "#### diachronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_2015 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_2015.wv.most_similar(LEX, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-malta",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_2020 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_2020.wv.most_similar(LEX, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-picnic",
   "metadata": {},
   "source": [
    "#### social"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_politics = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_politics.wv.most_similar(LEX, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_historians = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_anglo_saxon_historians.wv.most_similar(LEX, topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-transfer",
   "metadata": {},
   "source": [
    "## Obsolete stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-throw",
   "metadata": {},
   "source": [
    "### Gensim preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "coral-concert",
   "metadata": {},
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short"
   ]
  },
  {
   "cell_type": "raw",
   "id": "civic-elimination",
   "metadata": {},
   "source": [
    "# docs_clean = preprocess_documents(docs)\n",
    "# docs_clean = stem_text()\n",
    "#docs_clean = strip_tags(docs)\n",
    "docs_clean = strip_punctuation(docs_clean)\n",
    "docs_clean = strip_multiple_whitespaces(docs_clean)\n",
    "docs_clean = strip_numeric(docs_clean)\n",
    "docs_clean = remove_stopwords(docs_clean)\n",
    "docs_clean = strip_short(docs_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

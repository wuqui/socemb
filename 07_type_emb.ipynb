{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp type_emb\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-little",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-garage",
   "metadata": {},
   "source": [
    "# Type embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-wound",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-republican",
   "metadata": {},
   "outputs": [],
   "source": [
    "VECS_DIR = '~/promo/socemb/data/vecs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_spec = {'comm': 'askaconservative', 'year': 2019}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_spec = {'comm': 'askaconservative', 'year': 2020}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-maryland",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "from socemb.read_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-drill",
   "metadata": {},
   "source": [
    "## Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path_1 = get_fpath_subr_yr(model_1_spec['comm'], LIMIT, model_1_spec['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path_2 = get_fpath_subr_yr(model_2_spec['comm'], LIMIT, model_2_spec['year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_1 = read_comm_csv(f_path_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-worcester",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_2 = read_comm_csv(f_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-gathering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 97243 entries, 0 to 97242\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   body         97243 non-null  string        \n",
      " 1   created_utc  97243 non-null  datetime64[ns]\n",
      " 2   id           97243 non-null  string        \n",
      " 3   subreddit    97243 non-null  string        \n",
      "dtypes: datetime64[ns](1), string(3)\n",
      "memory usage: 3.7 MB\n"
     ]
    }
   ],
   "source": [
    "comments_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-lightning",
   "metadata": {},
   "source": [
    "## Split comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-bailey",
   "metadata": {},
   "source": [
    "### Split in temporal bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.assign(date = pd.to_datetime(\n",
    "    comments['created_utc'],\n",
    "    errors='coerce'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments[comments.date.dt.year == TIME]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-headquarters",
   "metadata": {},
   "source": [
    "### Split by communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments\\\n",
    "    .value_counts('subreddit')\\\n",
    "    .head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"politics\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.query('subreddit == \"AskHistorians\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-cancellation",
   "metadata": {},
   "source": [
    "## Pre-process comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def conv_to_lowerc(doc):\n",
    "    return doc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert conv_to_lowerc('Test') == 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def rm_punct(doc):\n",
    "    return re.sub(r'[^\\w\\s]+', ' ', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rm_punct('No-punctuation!') == 'No punctuation '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tokenize(doc):\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokenize('These are three-tokens')) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def detect_short_doc(doc, limit=10):\n",
    "    if len(doc) >= limit:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei'], 10) == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert detect_short_doc(['oans', 'zwoa', 'drei', 'viere', 'fuenfe', 'sechse', 'simme', 'achte', 'neine', 'zehne'], 10) == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def clean_docs(docs):\n",
    "    docs_clean = docs\\\n",
    "        .apply(conv_to_lowerc)\\\n",
    "        .apply(rm_punct)\\\n",
    "        .apply(tokenize)\\\n",
    "        .where(lambda x : x.apply(detect_short_doc) == False)\\\n",
    "        .dropna()    \n",
    "    return docs_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean_1 = clean_docs(comments_1['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_clean_2 = clean_docs(comments_2['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-suggestion",
   "metadata": {},
   "source": [
    "## Create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Corpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "    def __init__(self, docs_clean):\n",
    "        self.docs_clean = docs_clean\n",
    "\n",
    "    def __iter__(self):\n",
    "        for doc in self.docs_clean:\n",
    "            yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_1 = Corpus(docs_clean_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_2 = Corpus(docs_clean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-charge",
   "metadata": {},
   "source": [
    "## Train embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def train_emb(corpus, MIN_COUNT=5, SIZE=300, WORKERS=8, WINDOW=3):\n",
    "    model = Word2Vec(\n",
    "        corpus, \n",
    "        min_count=MIN_COUNT,\n",
    "        size=SIZE,\n",
    "        workers=WORKERS, \n",
    "        window=WINDOW\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = train_emb(corpus_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 s, sys: 223 ms, total: 45.2 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_2 = train_emb(corpus_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-insurance",
   "metadata": {},
   "source": [
    "## Evaluate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/20738: the\n",
      "1/20738: to\n",
      "2/20738: and\n",
      "3/20738: a\n",
      "4/20738: of\n",
      "5/20738: that\n",
      "6/20738: is\n",
      "7/20738: i\n",
      "8/20738: it\n",
      "9/20738: in\n"
     ]
    }
   ],
   "source": [
    "for index, word in enumerate(model_1.wv.index2word):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"{index}/{len(model_1.wv.index2word)}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-accuracy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.7230769991874695),\n",
       " ('woman', 0.7013160586357117),\n",
       " ('guy', 0.6996582746505737),\n",
       " ('someone', 0.6453497409820557),\n",
       " ('kid', 0.6225624084472656),\n",
       " ('baby', 0.6172075271606445),\n",
       " ('child', 0.6142028570175171),\n",
       " ('anyone', 0.6052091121673584),\n",
       " ('doctor', 0.5907690525054932),\n",
       " ('parent', 0.5853807926177979),\n",
       " ('fetus', 0.5693686604499817),\n",
       " ('politician', 0.5691657066345215),\n",
       " ('mother', 0.5550119876861572),\n",
       " ('somebody', 0.552615225315094),\n",
       " ('anybody', 0.5372178554534912),\n",
       " ('group', 0.5359237194061279),\n",
       " ('friend', 0.5345120429992676),\n",
       " ('citizen', 0.5333900451660156),\n",
       " ('lawyer', 0.5299038887023926),\n",
       " ('president', 0.528654932975769)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.wv.most_similar('person', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_rel_1 = 'person'\n",
    "lex_rel_2 = 'man'\n",
    "lex_unrel = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-handy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.723077"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_rel = model_1.wv.similarity(lex_rel_1, lex_rel_2)\n",
    "sim_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-bracelet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19468208"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_1 = model_1.wv.similarity(lex_rel_1, lex_unrel)\n",
    "sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-influence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1299858"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_unrel_2 = model_1.wv.similarity(lex_rel_2, lex_unrel)\n",
    "sim_unrel_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-injury",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sim_rel > sim_unrel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-newark",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.wv.save(f'{VECS_DIR}/model_1.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.wv.save(f'{VECS_DIR}/model_2.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-venture",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-wonder",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = KeyedVectors.load(f'{VECS_DIR}/model_1.kv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = KeyedVectors.load(f'{VECS_DIR}/model_2.kv', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_model(SUBREDDIT: str, YEAR: int, LIMIT: int = 100_000):\n",
    "    \"\"\"Load word2vec model from disk.\"\"\"\n",
    "    model = KeyedVectors.load(f'data/vecs/{SUBREDDIT}_{YEAR}_{LIMIT}.kv', mmap='r')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_vec_from_model(lex: str, model):\n",
    "    return model[lex]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-example",
   "metadata": {},
   "source": [
    "## Align models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-translator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_gensim_models(models, words=None):\n",
    "    \"\"\"\n",
    "    Returns the aligned/intersected models from a list of gensim word2vec models.\n",
    "    Generalized from original two-way intersection as seen above.\n",
    "    \n",
    "    Also updated to work with the most recent version of gensim\n",
    "    Requires reduce from functools\n",
    "    \n",
    "    In order to run this, make sure you run 'model.init_sims()' for each model before you input them for alignment.\n",
    "    \n",
    "    ##############################################\n",
    "    ORIGINAL DESCRIPTION\n",
    "    ##############################################\n",
    "    \n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocabs = [set(m.vocab.keys()) for m in models]\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = reduce((lambda vocab1,vocab2: vocab1&vocab2), vocabs)\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    \n",
    "    # This was generalized from:\n",
    "    # if not vocab_m1-common_vocab and not vocab_m2-common_vocab and not vocab_m3-common_vocab:\n",
    "    #   return (m1,m2,m3)\n",
    "    if all(not vocab-common_vocab for vocab in vocabs):\n",
    "        print(\"All identical!\")\n",
    "        return models\n",
    "        \n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: sum([m.vocab[w].count for m in models]),reverse=True)\n",
    "    \n",
    "    # Then for each model...\n",
    "    for m in models:\n",
    "        \n",
    "        # Replace old vectors_norm array with new one (with common vocab)\n",
    "        indices = [m.vocab[w].index for w in common_vocab]\n",
    "                \n",
    "        old_arr = m.vectors_norm\n",
    "                \n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.vectors_norm = m.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.vocab = new_vocab\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.init_sims(replace=True)\n",
    "model_2.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-27191591a0df>:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  models_ali = align_gensim_models([model_1.wv, model_2.wv])\n",
      "<ipython-input-48-a84bdbe9e880>:53: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  m.vectors_norm = m.syn0 = new_arr\n"
     ]
    }
   ],
   "source": [
    "models_ali = align_gensim_models([model_1.wv, model_2.wv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_ali = models_ali[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2_ali = models_ali[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-northern",
   "metadata": {},
   "source": [
    "## Measure distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-mechanics",
   "metadata": {},
   "source": [
    "### For `1` pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t1 = model_1['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_t2 = model_2['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t1.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-morrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert vec_t2.shape[0] == 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec_t1, vec_t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-casting",
   "metadata": {},
   "source": [
    "### For full vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distances(model_1, model_2):\n",
    "    dist_dict = {}\n",
    "    # will need to replace `vecs_t1.vocab` with a general vocab\n",
    "    for word in model_1.wv.vocab:\n",
    "        if word in model_2.wv.vocab:\n",
    "            dist_dict[word] = cosine(model_1[word], model_2[word])\n",
    "        else:\n",
    "            pass\n",
    "    dist_dict_df = pd.DataFrame(\n",
    "        data=dist_dict.items(), \n",
    "        columns=['word', 'dist']\n",
    "    )\\\n",
    "        .sort_values('dist', ascending=False)\n",
    "    return dist_dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-major",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-682d38aa6c6f>:4: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  for word in model_1.wv.vocab:\n",
      "<ipython-input-53-682d38aa6c6f>:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if word in model_2.wv.vocab:\n"
     ]
    }
   ],
   "source": [
    "dists = get_distances(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-albert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>virus</td>\n",
       "      <td>0.901883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>mask</td>\n",
       "      <td>0.898984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13665</th>\n",
       "      <td>subordinate</td>\n",
       "      <td>0.878960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>mail</td>\n",
       "      <td>0.862455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>masks</td>\n",
       "      <td>0.859398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15531</th>\n",
       "      <td>characterizing</td>\n",
       "      <td>0.837203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>cloth</td>\n",
       "      <td>0.825533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>rioters</td>\n",
       "      <td>0.823422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>usps</td>\n",
       "      <td>0.789642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15786</th>\n",
       "      <td>gutter</td>\n",
       "      <td>0.786883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15332</th>\n",
       "      <td>bog</td>\n",
       "      <td>0.779949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>language</td>\n",
       "      <td>0.779738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5407</th>\n",
       "      <td>contest</td>\n",
       "      <td>0.779108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13697</th>\n",
       "      <td>swell</td>\n",
       "      <td>0.774971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>charged</td>\n",
       "      <td>0.774869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>wearing</td>\n",
       "      <td>0.769502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>x200b</td>\n",
       "      <td>0.768461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>b</td>\n",
       "      <td>0.767424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16204</th>\n",
       "      <td>instigate</td>\n",
       "      <td>0.765337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4120</th>\n",
       "      <td>whistleblower</td>\n",
       "      <td>0.762012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word      dist\n",
       "1000            virus  0.901883\n",
       "792              mask  0.898984\n",
       "13665     subordinate  0.878960\n",
       "665              mail  0.862455\n",
       "908             masks  0.859398\n",
       "15531  characterizing  0.837203\n",
       "4837            cloth  0.825533\n",
       "2413          rioters  0.823422\n",
       "2607             usps  0.789642\n",
       "15786          gutter  0.786883\n",
       "15332             bog  0.779949\n",
       "1239         language  0.779738\n",
       "5407          contest  0.779108\n",
       "13697           swell  0.774971\n",
       "1954          charged  0.774869\n",
       "1199          wearing  0.769502\n",
       "435             x200b  0.768461\n",
       "1413                b  0.767424\n",
       "16204       instigate  0.765337\n",
       "4120    whistleblower  0.762012"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists.nlargest(20, 'dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-karma",
   "metadata": {},
   "source": [
    "### Inspect nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar('cancel', topn=20):\n",
    "        print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-mailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    i = 0\n",
    "    while i < 10:\n",
    "        for nb, dist in model.most_similar(lex):\n",
    "            if model.vocab[nb].count > 300:\n",
    "                print(f'{nb:<15}{model.vocab[nb].count:>6}')\n",
    "                i += 1\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-effects",
   "metadata": {},
   "source": [
    "### Temporal variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-arena",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in [model_1, model_2]:\n",
    "    for nb, dist in model.most_similar(lex, topn=10):\n",
    "        print(nb)\n",
    "    print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-validity",
   "metadata": {},
   "source": [
    "### Social variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = 'quarantine'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{COMM_1}\\n---')\n",
    "for nb, dist in model_1.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{COMM_2}\\n---')\n",
    "for nb, dist in model_2.most_similar(lex, topn=10):\n",
    "    print(nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-backup",
   "metadata": {},
   "source": [
    "## Obsolete stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-conversion",
   "metadata": {},
   "source": [
    "### Extract vector for target word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-audit",
   "metadata": {},
   "source": [
    "### Gensim preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dependent-injection",
   "metadata": {},
   "source": [
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_multiple_whitespaces, strip_numeric, remove_stopwords, strip_short"
   ]
  },
  {
   "cell_type": "raw",
   "id": "grave-immigration",
   "metadata": {},
   "source": [
    "# docs_clean = preprocess_documents(docs)\n",
    "# docs_clean = stem_text()\n",
    "#docs_clean = strip_tags(docs)\n",
    "docs_clean = strip_punctuation(docs_clean)\n",
    "docs_clean = strip_multiple_whitespaces(docs_clean)\n",
    "docs_clean = strip_numeric(docs_clean)\n",
    "docs_clean = remove_stopwords(docs_clean)\n",
    "docs_clean = strip_short(docs_clean)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
